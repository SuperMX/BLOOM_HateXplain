{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5ae02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.1.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface_hub) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->huggingface_hub) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# requirements:\n",
    "! pip install huggingface_hub\n",
    "! git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb95f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\Markus/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "#enter your API key, you can make one for free on HF\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a44384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceApi\n",
    "\n",
    "inference = InferenceApi(\"bigscience/bloom\",token=HfFolder.get_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3d452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def infer(prompt,\n",
    "          max_length = 32,\n",
    "          top_k = 0,\n",
    "          num_beams = 0,\n",
    "          no_repeat_ngram_size = 2,\n",
    "          top_p = 0.9,\n",
    "          seed=42,\n",
    "          temperature=0.7,\n",
    "          greedy_decoding = False,\n",
    "          return_full_text = False):\n",
    "    \n",
    "\n",
    "    top_k = None if top_k == 0 else top_k\n",
    "    do_sample = False if num_beams > 0 else not greedy_decoding\n",
    "    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n",
    "    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n",
    "    top_p = None if num_beams else top_p\n",
    "    early_stopping = None if num_beams is None else num_beams > 0\n",
    "\n",
    "    params = {\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"seed\": seed,\n",
    "        \"early_stopping\":early_stopping,\n",
    "        \"no_repeat_ngram_size\":no_repeat_ngram_size,\n",
    "        \"num_beams\":num_beams,\n",
    "        \"return_full_text\":return_full_text\n",
    "    }\n",
    "    \n",
    "    s = time.time()\n",
    "    response = inference(prompt, params=params)\n",
    "    #print(response)\n",
    "    proc_time = time.time()-s\n",
    "    #print(f\"Processing time was {proc_time} seconds\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8dccc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(sentence, rationale, label):\n",
    "    return \"\"\"\n",
    "    **Sentence:**\n",
    "    \"\"\"+sentence+\"\"\"\n",
    "    \n",
    "    **Rationale:**\n",
    "    \"\"\"+rationale+\"\"\"\n",
    "    \n",
    "    **Label:**\n",
    "    \"\"\"+label+\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def prompt_checkhate(sentence):\n",
    "    prompt = \"\"\"\n",
    "    \"\"\"+l(\"anyone consider maybe the missiles were oppressed us refugees looking for asylum in syria\",\"\",\"None\")+\"\"\"\n",
    "    \"\"\"+l(\"you are a faggot sir\",\"faggot\",\"Hate\")+\"\"\"\n",
    "    \"\"\"+l(\"...she says i will grab any woman by the pussy and fuck you brownies and fuck you fags and dykes us he a rapist racist homophobe transphobe you all how can you be so unpatriotic...\",\"dykes\",\"Hate\")+\"\"\"\n",
    "    \"\"\"+l(\"now i hate the term libtard but people who are afraid of violence joker movie could incite are faggots\",\"faggots\",\"Hate\")+\"\"\"\n",
    "    \"\"\"+l(\"another shit thread bumped up by the gab faggot\",\"faggot\",\"Hate\")+\"\"\"\n",
    "    \"\"\"+l(\"go to bed faggot\",\"faggot\",\"Hate\")+\"\"\"\n",
    "    \"\"\"+l(\"whoever made this thread has the big gay\",\"gay\",\"Hate\")+\"\"\"\n",
    "    \"\"\"+l(\"faggots who put such an extreme emphasis on bourgouise education are like retarded children or maybe more like a normie republican who do not understand nature vs nurture wtf is wrong w you\",\"faggots, retarded\",\"Hate\")+\"\"\"\n",
    "    **Sentence:**\n",
    "    \"\"\"+sentence+\"\"\"\n",
    "\n",
    "    **Rationale:**\n",
    "    \"\"\"\n",
    "    resp = infer(prompt)\n",
    "\n",
    "    string = resp[0]['generated_text']\n",
    "    answer = string.replace(prompt, \"\")\n",
    "    answer = \"\"\"\n",
    "    **Sentence:**\n",
    "    \"\"\"+sentence+\"\"\"\n",
    "\n",
    "    **Rationale:**\n",
    "    \"\"\"+answer\n",
    "    \n",
    "    # where is \"Label\"\n",
    "    start_index = answer.find(\"Label\")\n",
    "    if(start_index == -1):\n",
    "        print(\"ERROR: Prompt engine cannot interfere\")\n",
    "        return\n",
    "    # cut one \\n after   \n",
    "    cut = len(answer)\n",
    "    once_more = False\n",
    "    for i in range (len(answer)-1):\n",
    "        index = answer.find(\"\\n\", i, i+1)\n",
    "        if(index > start_index):\n",
    "            if(once_more):\n",
    "                cut = index\n",
    "                #print(\"cut is \"+str(cut)+\", start_index is \"+str(start_index))\n",
    "                break\n",
    "                \n",
    "            once_more = True\n",
    "            continue\n",
    "\n",
    "    answer = answer[0:cut]\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "173aa9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    **Sentence:**\n",
      "    take yo bitch ass back to the east\n",
      "\n",
      "    **Rationale:**\n",
      "    \n",
      "    \n",
      "    **Label:**\n",
      "    Hate\n"
     ]
    }
   ],
   "source": [
    "prompt_checkhate(\"take yo bitch ass back to the east\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1eeb2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    **Sentence:**\n",
      "    I love you you are so nice to me\n",
      "\n",
      "    **Rationale:**\n",
      "    \n",
      "    \n",
      "    **Label:**\n",
      "    None\n"
     ]
    }
   ],
   "source": [
    "prompt_checkhate(\"I love you you are so nice to me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2064c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Prompt engine cannot interfere\n"
     ]
    }
   ],
   "source": [
    "prompt_checkhate(\"you are a faggot sir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1243de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    **Sentence:**\n",
      "    this is what you nazi faggots do all day\n",
      "\n",
      "    **Rationale:**\n",
      "    \n",
      "    \n",
      "    **Label:**\n",
      "    Hate\n"
     ]
    }
   ],
   "source": [
    "prompt_checkhate(\"this is what you nazi faggots do all day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f1257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
